{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "atlantic-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    AdamW,\n",
    "    BertForSequenceClassification,\n",
    "    set_seed,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceramic-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "convenient-commonwealth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "oriented-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cls'] = None\n",
    "for idx, row in df.iterrows():\n",
    "    flag = False\n",
    "    for level_one_tag, level_two_tag_list in level_two_tag.items():\n",
    "        if row['二级标签'] in level_two_tag_list:\n",
    "            df['cls'][idx] = level_one_tag\n",
    "            flag = True\n",
    "            break\n",
    "    if flag is False:\n",
    "        print('二级标签<{}>未找到对应分类'.format(row['二级标签']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "wrapped-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id_map = {\n",
    "    '色情类': 0,\n",
    "    '广告类': 1,\n",
    "    '政治类': 2,\n",
    "    '辱骂类': 3,\n",
    "    '其他类': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rising-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/data-input/lichunyu/data/5cls.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "gentle-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "encouraging-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('/data-input/lichunyu/bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "lovely-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch(df):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for idx, row in df.iterrows():\n",
    "        encoded_dict = tokenizer(\n",
    "                            row['text'], \n",
    "                            add_special_tokens = True,\n",
    "                            truncation='longest_first',\n",
    "                            max_length = 150,\n",
    "                            padding = 'max_length',\n",
    "                            return_attention_mask = True,\n",
    "                            return_tensors = 'pt',\n",
    "                       )\n",
    "\n",
    "\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(df['label'].tolist())\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "    \n",
    "\n",
    "train_input_ids, train_attention_masks, train_labels = tokenize_batch(df_train)\n",
    "test_input_ids, test_attention_masks, test_labels= tokenize_batch(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bacterial-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "different-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            sampler = SequentialSampler(test_dataset),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "electoral-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 5e-5,\n",
    "                  eps = 1e-8\n",
    "                )\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "thick-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    return accuracy_score(labels_flat, pred_flat)\n",
    "\n",
    "def flat_f1(preds, labels):\n",
    "\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    return f1_score(labels_flat, pred_flat, average='micro')\n",
    "\n",
    "\n",
    "def format_time(elapsed):    \n",
    "\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "formal-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model)\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "every-traveler",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data-input/lichunyu/bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /data-input/lichunyu/bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    '/data-input/lichunyu/bert-base-chinese',\n",
    "    num_labels = 5,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bottom-theme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    16  of    117.    Elapsed: 0:00:04.\n",
      "  Batch    32  of    117.    Elapsed: 0:00:09.\n",
      "  Batch    48  of    117.    Elapsed: 0:00:13.\n",
      "  Batch    64  of    117.    Elapsed: 0:00:17.\n",
      "  Batch    80  of    117.    Elapsed: 0:00:22.\n",
      "  Batch    96  of    117.    Elapsed: 0:00:26.\n",
      "  Batch   112  of    117.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.46\n",
      "  Training epcoh took: 0:00:32\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "  F1: 0.92\n",
      "  Validation Loss: 0.22\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    16  of    117.    Elapsed: 0:00:04.\n",
      "  Batch    32  of    117.    Elapsed: 0:00:09.\n",
      "  Batch    48  of    117.    Elapsed: 0:00:13.\n",
      "  Batch    64  of    117.    Elapsed: 0:00:17.\n",
      "  Batch    80  of    117.    Elapsed: 0:00:22.\n",
      "  Batch    96  of    117.    Elapsed: 0:00:26.\n",
      "  Batch   112  of    117.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epcoh took: 0:00:32\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  F1: 0.96\n",
      "  Validation Loss: 0.15\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    16  of    117.    Elapsed: 0:00:04.\n",
      "  Batch    32  of    117.    Elapsed: 0:00:09.\n",
      "  Batch    48  of    117.    Elapsed: 0:00:13.\n",
      "  Batch    64  of    117.    Elapsed: 0:00:17.\n",
      "  Batch    80  of    117.    Elapsed: 0:00:22.\n",
      "  Batch    96  of    117.    Elapsed: 0:00:26.\n",
      "  Batch   112  of    117.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epcoh took: 0:00:32\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  F1: 0.96\n",
      "  Validation Loss: 0.16\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    16  of    117.    Elapsed: 0:00:04.\n",
      "  Batch    32  of    117.    Elapsed: 0:00:09.\n",
      "  Batch    48  of    117.    Elapsed: 0:00:13.\n",
      "  Batch    64  of    117.    Elapsed: 0:00:17.\n",
      "  Batch    80  of    117.    Elapsed: 0:00:22.\n",
      "  Batch    96  of    117.    Elapsed: 0:00:26.\n",
      "  Batch   112  of    117.    Elapsed: 0:00:30.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:00:31\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  F1: 0.96\n",
      "  Validation Loss: 0.15\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:02:17 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    print('')\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 16 == 0 and not step == 0:\n",
    "\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        input_ids = batch[0].to(device).to(torch.int64)\n",
    "        attention_mask = batch[1].to(device).to(torch.int64)\n",
    "        labels = batch[2].to(device).to(torch.int64)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        output = model(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask, \n",
    "                             labels=labels)\n",
    "\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print('')\n",
    "    print('  Average training loss: {0:.2f}'.format(avg_train_loss))\n",
    "    print('  Training epcoh took: {:}'.format(training_time))\n",
    "        \n",
    "    print('')\n",
    "    print('Running Validation...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    total_eval_f1 = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "\n",
    "        input_ids = batch[0].to(device).to(torch.int64)\n",
    "        attention_mask = batch[1].to(device).to(torch.int64)\n",
    "        labels = batch[2].to(device).to(torch.int64)\n",
    " \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            output = model(input_ids,\n",
    "                           attention_mask=attention_mask,\n",
    "                           labels=labels)\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "   \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        total_eval_f1 += flat_f1(logits, label_ids)\n",
    "        \n",
    "    avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "    print('  Accuracy: {0:.2f}'.format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_f1 = total_eval_f1 / len(test_dataloader)\n",
    "    print('  F1: {0:.2f}'.format(avg_val_f1))\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(test_dataloader)\n",
    "    \n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print('  Validation Loss: {0:.2f}'.format(avg_val_loss))\n",
    "    print('  Validation took: {:}'.format(validation_time))\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Val_F1' : avg_val_f1,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print('')\n",
    "print('Training complete!')\n",
    "\n",
    "print('Total training took {:} (h:mm:ss)'.format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "mature-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/data-input/lichunyu/models/bert-6301111-96-cls5.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "latin-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WEIGHTS_NAME\n",
    "output_model_file = os.path.join('/data-input/lichunyu/models/', WEIGHTS_NAME)\n",
    "torch.save(model.state_dict(), output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "expected-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id_map = {\n",
    "    '色情类': 0,\n",
    "    '广告类': 1,\n",
    "    '政治类': 2,\n",
    "    '辱骂类': 3,\n",
    "    '其他类': 4\n",
    "}\n",
    "id2label = {v: k for k, v in label2id_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "satisfactory-apparatus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2235, -1.3230, -0.6740, -0.7727,  3.4490]], device='cuda:0')\n",
      "==========类别==========\n",
      "其他类\n"
     ]
    }
   ],
   "source": [
    "x = '今天天气不错'\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# for i in test:\n",
    "encoded_dict = tokenizer(\n",
    "                x, \n",
    "                add_special_tokens = True,\n",
    "                truncation='longest_first',\n",
    "                max_length = 150,\n",
    "                padding = 'max_length',\n",
    "                return_attention_mask = True,\n",
    "                return_tensors = 'pt',\n",
    "           )\n",
    "input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "model.eval()\n",
    "with torch.no_grad():        \n",
    "\n",
    "    input_ids = input_ids.to(device).to(torch.int64)\n",
    "    attention_masks = attention_masks.to(device).to(torch.int64)\n",
    "    output = model(input_ids=input_ids,\n",
    "                   attention_mask=attention_masks)\n",
    "\n",
    "print(output.logits)\n",
    "print('==========类别==========')\n",
    "print(id2label[torch.argmax(output.logits).detach().cpu().numpy().tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "valid-optics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cls</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>318</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>547</td>\n",
       "      <td>547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>340</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text  cls\n",
       "label           \n",
       "0       199  199\n",
       "1       318  318\n",
       "2       547  547\n",
       "3       340  340"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "expected-remains",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/data-input/lichunyu/data/chat.txt', 'r') as f:\n",
    "    chat_dt = f.read().splitlines()\n",
    "    \n",
    "chat_dt_500 = chat_dt[:800]\n",
    "chat_dt_clean = [d for d in chat_dt_500 if len(d) > 6]\n",
    "len(chat_dt_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "welsh-evans",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cls</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>六人行 第1季 第01集 莫妮卡的新室友</td>\n",
       "      <td>其他</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>没什么好说的！ 他不过是我的同事！</td>\n",
       "      <td>其他</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>少来了，你和那个人一起出去！</td>\n",
       "      <td>其他</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>和你交往的男人一定有问题！</td>\n",
       "      <td>其他</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>打住，乔伊，嘴下留德</td>\n",
       "      <td>其他</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>真的?我不记得女人也会制造精子</td>\n",
       "      <td>其他</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>我们都知道那是个极大的挑战</td>\n",
       "      <td>其他</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>够了，你们两个，别吵了</td>\n",
       "      <td>其他</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>不，功劳全让她抢了,我也在场</td>\n",
       "      <td>其他</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>你该不会想用海伦威利克班奇盖勒吧?  因为这有虐待儿童之嫌</td>\n",
       "      <td>其他</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              text cls  label\n",
       "0             六人行 第1季 第01集 莫妮卡的新室友  其他      4\n",
       "1                没什么好说的！ 他不过是我的同事！  其他      4\n",
       "2                   少来了，你和那个人一起出去！  其他      4\n",
       "3                    和你交往的男人一定有问题！  其他      4\n",
       "4                       打住，乔伊，嘴下留德  其他      4\n",
       "..                             ...  ..    ...\n",
       "577                真的?我不记得女人也会制造精子  其他      4\n",
       "578                  我们都知道那是个极大的挑战  其他      4\n",
       "579                    够了，你们两个，别吵了  其他      4\n",
       "580                 不，功劳全让她抢了,我也在场  其他      4\n",
       "581  你该不会想用海伦威利克班奇盖勒吧?  因为这有虐待儿童之嫌  其他      4\n",
       "\n",
       "[582 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_chat = {\n",
    "    'text': chat_dt_clean,\n",
    "    'cls': len(chat_dt_clean)*['其他'],\n",
    "    'label': len(chat_dt_clean)*[4]\n",
    "}\n",
    "df_chat = pd.DataFrame(dt_chat)\n",
    "df_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "innocent-stuff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cls</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>什么烂剧  卧槽</td>\n",
       "      <td>辱骂类</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>妈的，颠覆三观的绿，看的想吐?</td>\n",
       "      <td>辱骂类</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>怪不得你们爱豆越活越娘炮，玩个游戏用力一点就叫的跟狗似的，还是去玩泥巴去吧</td>\n",
       "      <td>辱骂类</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>身边有一婊子弄的我实在无奈!怎么对付这种人kao!</td>\n",
       "      <td>辱骂类</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>关你球事？你晓得个锤子。</td>\n",
       "      <td>辱骂类</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>真的?我不记得女人也会制造精子</td>\n",
       "      <td>其他</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>我们都知道那是个极大的挑战</td>\n",
       "      <td>其他</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>够了，你们两个，别吵了</td>\n",
       "      <td>其他</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>不，功劳全让她抢了,我也在场</td>\n",
       "      <td>其他</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>你该不会想用海伦威利克班奇盖勒吧?  因为这有虐待儿童之嫌</td>\n",
       "      <td>其他</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2338 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      text  cls label\n",
       "0                                 什么烂剧  卧槽  辱骂类     3\n",
       "1                          妈的，颠覆三观的绿，看的想吐?  辱骂类     3\n",
       "2    怪不得你们爱豆越活越娘炮，玩个游戏用力一点就叫的跟狗似的，还是去玩泥巴去吧  辱骂类     3\n",
       "3                身边有一婊子弄的我实在无奈!怎么对付这种人kao!  辱骂类     3\n",
       "4                             关你球事？你晓得个锤子。  辱骂类     3\n",
       "..                                     ...  ...   ...\n",
       "577                        真的?我不记得女人也会制造精子   其他     4\n",
       "578                          我们都知道那是个极大的挑战   其他     4\n",
       "579                            够了，你们两个，别吵了   其他     4\n",
       "580                         不，功劳全让她抢了,我也在场   其他     4\n",
       "581          你该不会想用海伦威利克班奇盖勒吧?  因为这有虐待儿童之嫌   其他     4\n",
       "\n",
       "[2338 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df, df_chat])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "homeless-turtle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-logging",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}